{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My husband is a born shopper. He loves to look...</td>\n",
       "      <td>husband born shopper love look thing touch lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tea drinking was common in China for nearly on...</td>\n",
       "      <td>tea drinking common china nearly one thousand ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there was a scholar who want...</td>\n",
       "      <td>upon time scholar wanted gain knowledge day ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  My husband is a born shopper. He loves to look...   \n",
       "1  Tea drinking was common in China for nearly on...   \n",
       "2  Once upon a time, there was a scholar who want...   \n",
       "\n",
       "                                      clean_document  \n",
       "0  husband born shopper love look thing touch lik...  \n",
       "1  tea drinking common china nearly one thousand ...  \n",
       "2  upon time scholar wanted gain knowledge day ev...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"..\\input\\ingested_data\\cleaned_train_documents.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top2Vec Model on Clean Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american spend free time various way america country sport hunting fishing swimming team sport like baseball football million american watch favorite sport television also like play community orchestra make film recording go camping visit museum attend lecture travel garden read join hundred activity people also enjoy building thing home sewing clothes even making photograph thing fun well economy much american enjoy free time country time self improvement country million adult continue education chiefly going school evening free time expense added time spent personal activity american devote great amount time varied need community many hospital school library museum park community center organization assist poor depend many hour citizen devote activity often without pay several answer idea cooperating sharing responsibility one another benefit old country country first founded necessary settler work together live crossed dangerous sea risked struggle political religious freedom remains among many american distrust central government people still prefer thing within community rather give government control sometimes people offer time wish accomplish something money paid something benefit entire community true people use truly interested work learning experience matter reason hundred thousand called leisure hour put hard unpaid work one another community need'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df.clean_document.tolist()\n",
    "docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top2Vec\n",
      "\n",
      "    Creates jointly embedded topic, document and word vectors.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    documents: List of str\n",
      "        Input corpus, should be a list of strings.\n",
      "\n",
      "    min_count: int (Optional, default 50)\n",
      "        Ignores all words with total frequency lower than this. For smaller\n",
      "        corpora a smaller min_count will be necessary.\n",
      "\n",
      "    topic_merge_delta: float (default 0.1)\n",
      "        Merges topic vectors which have a cosine distance smaller than\n",
      "        topic_merge_delta using dbscan. The epsilon parameter of dbscan is\n",
      "        set to the topic_merge_delta.\n",
      "\n",
      "    ngram_vocab: bool (Optional, default False)\n",
      "        Add phrases to topic descriptions.\n",
      "\n",
      "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
      "        to the vocabulary.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    ngram_vocab_args: dict (Optional, default None)\n",
      "        Pass custom arguments to gensim phrases.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    embedding_model: string or callable\n",
      "        This will determine which model is used to generate the document and\n",
      "        word embeddings. The valid string options are:\n",
      "\n",
      "            * doc2vec\n",
      "            * universal-sentence-encoder\n",
      "            * universal-sentence-encoder-large\n",
      "            * universal-sentence-encoder-multilingual\n",
      "            * universal-sentence-encoder-multilingual-large\n",
      "            * distiluse-base-multilingual-cased\n",
      "            * all-MiniLM-L6-v2\n",
      "            * paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "        For large data sets and data sets with very unique vocabulary doc2vec\n",
      "        could produce better results. This will train a doc2vec model from\n",
      "        scratch. This method is language agnostic. However multiple languages\n",
      "        will not be aligned.\n",
      "\n",
      "        Using the universal sentence encoder options will be much faster since\n",
      "        those are pre-trained and efficient models. The universal sentence\n",
      "        encoder options are suggested for smaller data sets. They are also\n",
      "        good options for large data sets that are in English or in languages\n",
      "        covered by the multilingual model. It is also suggested for data sets\n",
      "        that are multilingual.\n",
      "\n",
      "        For more information on universal-sentence-encoder options visit:\n",
      "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
      "\n",
      "        The SBERT pre-trained sentence transformer options are\n",
      "        distiluse-base-multilingual-cased,\n",
      "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
      "\n",
      "        The distiluse-base-multilingual-cased and\n",
      "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
      "        datasets and languages that are not\n",
      "        covered by the multilingual universal sentence encoder. The\n",
      "        transformer is significantly slower than the universal sentence\n",
      "        encoder options(except for the large options).\n",
      "\n",
      "        For more information on SBERT options visit:\n",
      "        https://www.sbert.net/docs/pretrained_models.html\n",
      "\n",
      "        If passing a callable embedding_model note that it will not be saved\n",
      "        when saving a top2vec model. After loading such a saved top2vec model\n",
      "        the set_embedding_model method will need to be called and the same\n",
      "        embedding_model callable used during training must be passed to it.\n",
      "\n",
      "    embedding_model_path: string (Optional)\n",
      "        Pre-trained embedding models will be downloaded automatically by\n",
      "        default. However they can also be uploaded from a file that is in the\n",
      "        location of embedding_model_path.\n",
      "\n",
      "        Warning: the model at embedding_model_path must match the\n",
      "        embedding_model parameter type.\n",
      "\n",
      "    embedding_batch_size: int (default=32)\n",
      "        Batch size for documents being embedded.\n",
      "\n",
      "    split_documents: bool (default False)\n",
      "        If set to True, documents will be split into parts before embedding.\n",
      "        After embedding the multiple document part embeddings will be averaged\n",
      "        to create a single embedding per document. This is useful when documents\n",
      "        are very large or when the embedding model has a token limit.\n",
      "\n",
      "        Document chunking or a senticizer can be used for document splitting.\n",
      "\n",
      "    document_chunker: string or callable (default 'sequential')\n",
      "        This will break the document into chunks. The valid string options are:\n",
      "\n",
      "            * sequential\n",
      "            * random\n",
      "\n",
      "        The sequential chunker will split the document into chunks of specified\n",
      "        length and ratio of overlap. This is the recommended method.\n",
      "\n",
      "        The random chunking option will take random chunks of specified length\n",
      "        from the document. These can overlap and should be thought of as\n",
      "        sampling chunks with replacement from the document.\n",
      "\n",
      "        If a callable is passed it must take as input a list of tokens of\n",
      "        a document and return a list of strings representing the resulting\n",
      "        document chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    chunk_length: int (default 100)\n",
      "        The number of tokens per document chunk if using the document chunker\n",
      "        string options.\n",
      "\n",
      "    max_num_chunks: int (Optional)\n",
      "        The maximum number of chunks generated per document if using the\n",
      "        document chunker string options.\n",
      "\n",
      "    chunk_overlap_ratio: float (default 0.5)\n",
      "        Only applies to the 'sequential' document chunker.\n",
      "\n",
      "        Fraction of overlapping tokens between sequential chunks. A value of\n",
      "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
      "        previous chunk.\n",
      "\n",
      "    chunk_len_coverage_ratio: float (default 1.0)\n",
      "        Only applies to the 'random' document chunker option.\n",
      "\n",
      "        Proportion of token length that will be covered by chunks. Default\n",
      "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
      "        the document. This does not mean all tokens will be covered since\n",
      "        chunks can be overlapping.\n",
      "\n",
      "    sentencizer: callable (Optional)\n",
      "        A sentincizer callable can be passed. The input should be a string\n",
      "        representing the document and the output should be a list of strings\n",
      "        representing the document sentence chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    speed: string (Optional, default 'learn')\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        It will determine how fast the model takes to train. The\n",
      "        fast-learn option is the fastest and will generate the lowest quality\n",
      "        vectors. The learn option will learn better quality vectors but take\n",
      "        a longer time to train. The deep-learn option will learn the best\n",
      "        quality vectors but will take significant time to train. The valid\n",
      "        string speed options are:\n",
      "        \n",
      "            * fast-learn\n",
      "            * learn\n",
      "            * deep-learn\n",
      "\n",
      "    use_corpus_file: bool (Optional, default False)\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        Setting use_corpus_file to True can sometimes provide speedup for\n",
      "        large datasets when multiple worker threads are available. Documents\n",
      "        are still passed to the model as a list of str, the model will create\n",
      "        a temporary corpus file for training.\n",
      "\n",
      "    document_ids: List of str, int (Optional)\n",
      "        A unique value per document that will be used for referring to\n",
      "        documents in search results. If ids are not given to the model, the\n",
      "        index of each document in the original corpus will become the id.\n",
      "\n",
      "    keep_documents: bool (Optional, default True)\n",
      "        If set to False documents will only be used for training and not saved\n",
      "        as part of the model. This will reduce model size. When using search\n",
      "        functions only document ids will be returned, not the actual\n",
      "        documents.\n",
      "\n",
      "    workers: int (Optional)\n",
      "        The amount of worker threads to be used in training the model. Larger\n",
      "        amount will lead to faster training.\n",
      "    \n",
      "    tokenizer: callable (Optional, default None)\n",
      "        Override the default tokenization method. If None then\n",
      "        gensim.utils.simple_preprocess will be used.\n",
      "\n",
      "        Tokenizer must take a document and return a list of tokens.\n",
      "\n",
      "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
      "        If using an embedding model other than doc2vec, use the model's\n",
      "        tokenizer for document embedding. If set to True the tokenizer, either\n",
      "        default or passed callable will be used to tokenize the text to\n",
      "        extract the vocabulary for word embedding.\n",
      "\n",
      "    umap_args: dict (Optional, default None)\n",
      "        Pass custom arguments to UMAP.\n",
      "\n",
      "    hdbscan_args: dict (Optional, default None)\n",
      "        Pass custom arguments to HDBSCAN.\n",
      "    \n",
      "    verbose: bool (Optional, default True)\n",
      "        Whether to print status data during training.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 12:24:39,552 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 12:25:07,332 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-07-04 13:01:37,974 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-07-04 13:02:50,938 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-07-04 13:03:01,280 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(documents=docs, \n",
    "                speed='deep-learn',\n",
    "                workers=8) # embedding_model = 'all-MiniLM-L6-v2' or 'universal-sentence-encoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[953 749 576 555 474 423 379 338 320 313 298 297 272 271 266 263 262 252\n",
      " 250 241 220 220 216 214 211 208 206 200 199 196 193 193 188 168 167 163\n",
      " 160 156 154 153 152 152 151 150 148 147 144 140 139 137 136 136 134 133\n",
      " 133 131 131 128 128 126 125 121 120 120 120 118 117 116 111 109 109 108\n",
      " 108 107 107 103 103 102 102 101 100  99  96  96  95  95  95  94  94  93\n",
      "  92  92  91  91  88  87  87  86  86  85  83  83  80  79  79  79  78  78\n",
      "  78  78  77  77  77  77  77  76  76  76  76  75  74  73  73  72  71  70\n",
      "  69  68  68  67  67  67  66  66  66  64  63  62  62  61  61  61  60  60\n",
      "  59  59  59  58  58  58  57  57  56  55  55  55  54  54  54  54  53  52\n",
      "  52  52  51  51  51  50  50  50  49  49  48  48  48  47  47  47  47  46\n",
      "  46  46  46  45  45  45  45  44  44  44  44  44  43  43  43  42  42  42\n",
      "  42  41  41  41  41  41  40  40  39  38  38  36  36  35  34  33  33  33\n",
      "  33  32  32  31  30  29  28  27]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25139, 25139)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(topic_sizes), df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223]\n"
     ]
    }
   ],
   "source": [
    "print(topic_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_words, word_scores, topic_nums = model.get_topics()\n",
    "\n",
    "# for words, scores, topic_num in zip(topic_words, word_scores, topic_nums):\n",
    "#     print(f\"Topic No. {topic_num}\")\n",
    "#     print(f\"Words: {words[:15]}\\n\") # top 15 words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic No. 0\n",
      "Words: ['policeman' 'sir' 'angrily' 'man' 'madam' 'taxi' 'drove' 'answered'\n",
      " 'driver' 'sorry' 'parked' 'sank' 'officer' 'paid' 'thief' 'stanley'\n",
      " 'waited' 'arrest' 'cane' 'wallet' 'walked' 'assured' 'banker' 'door'\n",
      " 'merlin' 'buck' 'woke' 'swallowed' 'annie' 'doorway' 'waiter' 'bos'\n",
      " 'opened' 'wife' 'hurry' 'napoleon' 'burst' 'stopped' 'repaired' 'license'\n",
      " 'servant' 'faith' 'prayed' 'judging' 'crossing' 'looked' 'pedestrian'\n",
      " 'beggar' 'stood' 'merry']\n",
      "Topic No. 1\n",
      "Words: ['happiness' 'friendship' 'desire' 'positive' 'negative' 'happier'\n",
      " 'attitude' 'relationship' 'feeling' 'unhappy' 'self' 'confidence'\n",
      " 'esteem' 'feel' 'accept' 'accomplishment' 'respect' 'honest' 'fail'\n",
      " 'failure' 'success' 'achieve' 'goal' 'meaningful' 'honesty' 'realize'\n",
      " 'truly' 'succeed' 'shyness' 'others' 'accepting' 'arise' 'achieving'\n",
      " 'person' 'understanding' 'pleasure' 'trusted' 'satisfaction' 'accomplish'\n",
      " 'loyalty' 'unhappiness' 'overcome' 'guilt' 'personality' 'emotion'\n",
      " 'admit' 'psychologist' 'focusing' 'shy' 'expectation']\n"
     ]
    }
   ],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "\n",
    "for words, scores, topic_num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(f\"Topic No. {topic_num}\")\n",
    "    print(f\"Words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dinosaur', 'dna', 'fossil', 'evolution', 'evolutionary',\n",
       "       'extinct', 'skeleton', 'evolved', 'specie', 'ancestor',\n",
       "       'preserved', 'extinction', 'genetic', 'ape', 'discovery',\n",
       "       'scientist', 'bone', 'human', 'creature', 'biologist',\n",
       "       'scientific', 'evidence', 'mankind', 'distinct', 'gene', 'biology',\n",
       "       'existed', 'asteroid', 'animal', 'theory', 'organism', 'journal',\n",
       "       'mammal', 'biological', 'origin', 'ancient', 'darwin', 'existence',\n",
       "       'structure', 'discovered', 'previously', 'ecosystem', 'shark',\n",
       "       'being', 'proof', 'identical', 'earliest', 'leap', 'observation',\n",
       "       'reproduce'], dtype='<U15')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic No.: 100\n",
      "Topic Keywords:\n",
      "['dinosaur' 'dna' 'fossil' 'evolution' 'evolutionary' 'extinct' 'skeleton'\n",
      " 'evolved' 'specie' 'ancestor' 'preserved' 'extinction' 'genetic' 'ape'\n",
      " 'discovery' 'scientist' 'bone' 'human' 'creature' 'biologist'\n",
      " 'scientific' 'evidence' 'mankind' 'distinct' 'gene' 'biology' 'existed'\n",
      " 'asteroid' 'animal' 'theory' 'organism' 'journal' 'mammal' 'biological'\n",
      " 'origin' 'ancient' 'darwin' 'existence' 'structure' 'discovered'\n",
      " 'previously' 'ecosystem' 'shark' 'being' 'proof' 'identical' 'earliest'\n",
      " 'leap' 'observation' 'reproduce']\n",
      "\n",
      "---------------\n",
      "Document: 21370, Score: 0.7427197694778442\n",
      "-----------\n",
      "walt disney born created mickey mouse made famous died work dream die people world enjoy mickey mouse cartoon walt disney man easily one summer wanted job post office told young went home drew line face put father suit hat went back office told got job finally later life mr disney dream wanted build new kind amusement park would clean beautiful child could play happily adult could live nice hotel would fun people age called disneyland engineer told impossible dream family friend thought mad mr disney give idea disneyland opened public became successful amusement park usa walt disney dream came true\n",
      "-----------\n",
      "\n",
      "Document: 11105, Score: 0.7102022171020508\n",
      "-----------\n",
      "hong kong sept hong kong disneyland th theme park walt disney co opened monday morning chinese vice president zeng qinghong donald mg yam kuen chief executive hong kong special administrative region hksar tung chee hwa attended opening hong kong disneyland project worth billion u dollar jointly funded walt disney co hksar government people attended opening park estimate attract million visitor opening year expected draw million annually year percent visitor expected come mainland disney said walt disney official claimed choosing hong kong first place china build disneyland ba three major reason hong kong people wonderful hong kong beautiful city hong kong richest city china hong kong disneyland first disney theme park modeled osrly first disneyland california jay rasulo president walt disney park resort said hong kong disneyland smaller park hectaresa fact disney try point thousand guest got sneak peak park past month complained small disney plan expand project announced construction began disney part tokyo paris state prefix st californiaand florida company confirmed disney talking government shanghai opening park would open least\n",
      "-----------\n",
      "\n",
      "Document: 19215, Score: 0.7049615383148193\n",
      "-----------\n",
      "walt disney began make cartoon movie young much money always enough eat one day mouse ran near desk worked small office would like pet disney asked mouse caught mouse kept pet year later disney decided make cartoon making cartoon mouse named mortimer told wife mortimer mouse think mickey mouse would better name said right disney agreed made many mickey mouse cartoon people world saw mickey loved mickey mouse made disney famous come donald duck goofy dog others disney began make full length cartoon made cartoon movie television million child watched show every week california real boat castle train mountain river one beautiful park million people came disneyland died world forget quickly mickey mouse cartoon help u remember\n",
      "-----------\n",
      "\n",
      "Document: 23882, Score: 0.6975917220115662\n",
      "-----------\n",
      "secret world disneyland cat easy keep park clean every night closing time cat come park pest park nice worker park light walt disney apartment park light house people would know owner park today light always stay honor hidden mickey disneyland mickey mouse logo everywhere however hundred hidden mickey park often different find one know many hidden mickey park\n",
      "-----------\n",
      "\n",
      "Document: 10309, Score: 0.6882443428039551\n",
      "-----------\n",
      "magic kingdom first theme park walt disney world opening disney world theme park open day year although opening closing time park different traveling without kid try visit school day avoid largest crowd need visit school vacation try avoid week christmas new year fourth july staying disney world hotel avoid visiting magic kingdom extra magic hour day disney hotel guest get park early day wait time visitor arrive magic kingdom normal opening time buy walt disney world ticket online disney world website advice picking right ticket see guide disney world ticket also need call advance make lunch dinner reservation disney accepts reservation wdw dine day advance time go quickly call six month trip get lunch reservation castle recommend lunch cinderella royal table tour plan arrive magic kingdom front gate park open morning check magic kingdom opening time disney world website keep mind reach magic kingdom must park disney world transportation ticket center parking lot ride tram ttc take boat across seven sea lagoon magic kingdom take extra time get give extra hour summer half hour school year\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOPIC_NUM = 100\n",
    "print(f\"Topic No.: {TOPIC_NUM}\")\n",
    "\n",
    "print(f\"Topic Keywords:\\n{model.get_topics(TOPIC_NUM)[0][-1]}\\n\")\n",
    "print(\"---------------\")\n",
    "\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=TOPIC_NUM, num_docs=5)\n",
    "\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Magic Kingdom was the first theme park at Walt Disney World, opening in 1971. All Disney World theme parks are open 365 days a year, although opening and closing times for each park are different. If you are traveling without kids, try to visit on a school day to avoid the largest crowds. If you need to visit during a school vacation, try to avoid the week between Christmas and New Year's and the Fourth of July.\n",
      "If you are not staying at a Disney World hotel, avoid visiting the Magic Kingdom on its Extra Magic Hours days, as Disney's hotel guests get into the park early on those days,  _ wait times for visitors who arrive at the Magic Kingdom's normal opening time.\n",
      "Buy your Walt Disney World tickets online at Disney World's website. For advice on picking the right ticket, see our guide to Disney World tickets.\n",
      "You will also need to call in advance to make lunch and/or dinner reservations . Disney accepts reservations, through 1-407-WDW-DINE, up to 180 days in advance. Times do go quickly, so you should call six months before your trip to get a lunch reservation in the castle. We recommend you should have your lunch at Cinderella's Royal Table for your tour.\n",
      "Plan to arrive at the Magic Kingdom's front gate before the park opens in the morning. You can check the Magic Kingdom's opening time through Disney World's website.\n",
      "Keep in mind that to reach the Magic Kingdom, you must park at Disney World's Transportation and Ticket Center parking lot, ride at tram  to the TTC, then take a boat across the Seven Seas Lagoon to the Magic Kingdom. It will take you extra time to get there. Give yourself an extra hour in the summer and half-hour during the school year.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[10309, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../output/saved_models/top2vec_model_on_clean_documents\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top2Vec Model on Original Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over the last 30 years, Bangkok, once a small fishing village, has transformed into a rich, concrete, high-rise city that it is today. The spreading metropolis and its population of 12 million now produces 35 per cent of Thailand's economic wealth.\n",
      "As a magnet for foreign companies, Bangkok attracts many overseas managers and business people from different fields, including tourism, automobiles and electronics. The city's population of foreigners is in the high hundreds of thousands, with tens of thousands of Japanese, Chinese and western employees working alongside hundreds of thousands of Burmese who mostly do unskilled jobs shunned by Thais.\n",
      "For those used to the good life, the variety and quality of the city's food is a key attraction, says one US manager, before listing many of his favourite Italian, Mexican and, of course, Thai restaurants. Most offer quality meals for less than the cost of a takeaway sandwich in London.\n",
      "Great choice and value can be found in Bangkok's other attractions, too. For overseas business people who enjoy shopping in luxury and air-conditioned comfort, the city has hundreds of modern shopping malls. Some foreigners, however, prefer the charms of Chatuchak Market, where anything can be bought at a good price by the skilled bargainer.\n",
      "When the time comes to talk business many overseas business people prefer to move out of the markets and onto the golf course. Thailand has thousands of courses, which can provide a welcome break from the busy and noisy city life. But most business people go to the golf course because it's the perfect place to discuss the next big deal.\n",
      "Because of the fast-paced life some foreign business people see Bangkok as a place to stay for the short term, rather than a lifetime. Australian computer software designer Sarah Huang is seven months pregnant but still working full-time in her Bangkok office. She says the city is \"definitely a place I want to stay for the next five, ten years\". Nannies and home help are affordable, but high fees for quality secondary education have convinced Ms. Huang to return to Australia when her child reaches high school age.\n"
     ]
    }
   ],
   "source": [
    "docs = df.document.tolist()\n",
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Top2Vec\n",
      "\n",
      "    Creates jointly embedded topic, document and word vectors.\n",
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    documents: List of str\n",
      "        Input corpus, should be a list of strings.\n",
      "\n",
      "    min_count: int (Optional, default 50)\n",
      "        Ignores all words with total frequency lower than this. For smaller\n",
      "        corpora a smaller min_count will be necessary.\n",
      "\n",
      "    topic_merge_delta: float (default 0.1)\n",
      "        Merges topic vectors which have a cosine distance smaller than\n",
      "        topic_merge_delta using dbscan. The epsilon parameter of dbscan is\n",
      "        set to the topic_merge_delta.\n",
      "\n",
      "    ngram_vocab: bool (Optional, default False)\n",
      "        Add phrases to topic descriptions.\n",
      "\n",
      "        Uses gensim phrases to find common phrases in the corpus and adds them\n",
      "        to the vocabulary.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    ngram_vocab_args: dict (Optional, default None)\n",
      "        Pass custom arguments to gensim phrases.\n",
      "\n",
      "        For more information visit:\n",
      "        https://radimrehurek.com/gensim/models/phrases.html\n",
      "\n",
      "    embedding_model: string or callable\n",
      "        This will determine which model is used to generate the document and\n",
      "        word embeddings. The valid string options are:\n",
      "\n",
      "            * doc2vec\n",
      "            * universal-sentence-encoder\n",
      "            * universal-sentence-encoder-large\n",
      "            * universal-sentence-encoder-multilingual\n",
      "            * universal-sentence-encoder-multilingual-large\n",
      "            * distiluse-base-multilingual-cased\n",
      "            * all-MiniLM-L6-v2\n",
      "            * paraphrase-multilingual-MiniLM-L12-v2\n",
      "\n",
      "        For large data sets and data sets with very unique vocabulary doc2vec\n",
      "        could produce better results. This will train a doc2vec model from\n",
      "        scratch. This method is language agnostic. However multiple languages\n",
      "        will not be aligned.\n",
      "\n",
      "        Using the universal sentence encoder options will be much faster since\n",
      "        those are pre-trained and efficient models. The universal sentence\n",
      "        encoder options are suggested for smaller data sets. They are also\n",
      "        good options for large data sets that are in English or in languages\n",
      "        covered by the multilingual model. It is also suggested for data sets\n",
      "        that are multilingual.\n",
      "\n",
      "        For more information on universal-sentence-encoder options visit:\n",
      "        https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
      "\n",
      "        The SBERT pre-trained sentence transformer options are\n",
      "        distiluse-base-multilingual-cased,\n",
      "        paraphrase-multilingual-MiniLM-L12-v2, and all-MiniLM-L6-v2.\n",
      "\n",
      "        The distiluse-base-multilingual-cased and\n",
      "        paraphrase-multilingual-MiniLM-L12-v2 are suggested for multilingual\n",
      "        datasets and languages that are not\n",
      "        covered by the multilingual universal sentence encoder. The\n",
      "        transformer is significantly slower than the universal sentence\n",
      "        encoder options(except for the large options).\n",
      "\n",
      "        For more information on SBERT options visit:\n",
      "        https://www.sbert.net/docs/pretrained_models.html\n",
      "\n",
      "        If passing a callable embedding_model note that it will not be saved\n",
      "        when saving a top2vec model. After loading such a saved top2vec model\n",
      "        the set_embedding_model method will need to be called and the same\n",
      "        embedding_model callable used during training must be passed to it.\n",
      "\n",
      "    embedding_model_path: string (Optional)\n",
      "        Pre-trained embedding models will be downloaded automatically by\n",
      "        default. However they can also be uploaded from a file that is in the\n",
      "        location of embedding_model_path.\n",
      "\n",
      "        Warning: the model at embedding_model_path must match the\n",
      "        embedding_model parameter type.\n",
      "\n",
      "    embedding_batch_size: int (default=32)\n",
      "        Batch size for documents being embedded.\n",
      "\n",
      "    split_documents: bool (default False)\n",
      "        If set to True, documents will be split into parts before embedding.\n",
      "        After embedding the multiple document part embeddings will be averaged\n",
      "        to create a single embedding per document. This is useful when documents\n",
      "        are very large or when the embedding model has a token limit.\n",
      "\n",
      "        Document chunking or a senticizer can be used for document splitting.\n",
      "\n",
      "    document_chunker: string or callable (default 'sequential')\n",
      "        This will break the document into chunks. The valid string options are:\n",
      "\n",
      "            * sequential\n",
      "            * random\n",
      "\n",
      "        The sequential chunker will split the document into chunks of specified\n",
      "        length and ratio of overlap. This is the recommended method.\n",
      "\n",
      "        The random chunking option will take random chunks of specified length\n",
      "        from the document. These can overlap and should be thought of as\n",
      "        sampling chunks with replacement from the document.\n",
      "\n",
      "        If a callable is passed it must take as input a list of tokens of\n",
      "        a document and return a list of strings representing the resulting\n",
      "        document chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    chunk_length: int (default 100)\n",
      "        The number of tokens per document chunk if using the document chunker\n",
      "        string options.\n",
      "\n",
      "    max_num_chunks: int (Optional)\n",
      "        The maximum number of chunks generated per document if using the\n",
      "        document chunker string options.\n",
      "\n",
      "    chunk_overlap_ratio: float (default 0.5)\n",
      "        Only applies to the 'sequential' document chunker.\n",
      "\n",
      "        Fraction of overlapping tokens between sequential chunks. A value of\n",
      "        0 will result i no overlap, where as 0.5 will overlap half of the\n",
      "        previous chunk.\n",
      "\n",
      "    chunk_len_coverage_ratio: float (default 1.0)\n",
      "        Only applies to the 'random' document chunker option.\n",
      "\n",
      "        Proportion of token length that will be covered by chunks. Default\n",
      "        value of 1.0 means chunk lengths will add up to number of tokens of\n",
      "        the document. This does not mean all tokens will be covered since\n",
      "        chunks can be overlapping.\n",
      "\n",
      "    sentencizer: callable (Optional)\n",
      "        A sentincizer callable can be passed. The input should be a string\n",
      "        representing the document and the output should be a list of strings\n",
      "        representing the document sentence chunks.\n",
      "\n",
      "        Only one of document_chunker or sentincizer should be used.\n",
      "\n",
      "    speed: string (Optional, default 'learn')\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        It will determine how fast the model takes to train. The\n",
      "        fast-learn option is the fastest and will generate the lowest quality\n",
      "        vectors. The learn option will learn better quality vectors but take\n",
      "        a longer time to train. The deep-learn option will learn the best\n",
      "        quality vectors but will take significant time to train. The valid\n",
      "        string speed options are:\n",
      "        \n",
      "            * fast-learn\n",
      "            * learn\n",
      "            * deep-learn\n",
      "\n",
      "    use_corpus_file: bool (Optional, default False)\n",
      "\n",
      "        This parameter is only used when using doc2vec as embedding_model.\n",
      "\n",
      "        Setting use_corpus_file to True can sometimes provide speedup for\n",
      "        large datasets when multiple worker threads are available. Documents\n",
      "        are still passed to the model as a list of str, the model will create\n",
      "        a temporary corpus file for training.\n",
      "\n",
      "    document_ids: List of str, int (Optional)\n",
      "        A unique value per document that will be used for referring to\n",
      "        documents in search results. If ids are not given to the model, the\n",
      "        index of each document in the original corpus will become the id.\n",
      "\n",
      "    keep_documents: bool (Optional, default True)\n",
      "        If set to False documents will only be used for training and not saved\n",
      "        as part of the model. This will reduce model size. When using search\n",
      "        functions only document ids will be returned, not the actual\n",
      "        documents.\n",
      "\n",
      "    workers: int (Optional)\n",
      "        The amount of worker threads to be used in training the model. Larger\n",
      "        amount will lead to faster training.\n",
      "    \n",
      "    tokenizer: callable (Optional, default None)\n",
      "        Override the default tokenization method. If None then\n",
      "        gensim.utils.simple_preprocess will be used.\n",
      "\n",
      "        Tokenizer must take a document and return a list of tokens.\n",
      "\n",
      "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
      "        If using an embedding model other than doc2vec, use the model's\n",
      "        tokenizer for document embedding. If set to True the tokenizer, either\n",
      "        default or passed callable will be used to tokenize the text to\n",
      "        extract the vocabulary for word embedding.\n",
      "\n",
      "    umap_args: dict (Optional, default None)\n",
      "        Pass custom arguments to UMAP.\n",
      "\n",
      "    hdbscan_args: dict (Optional, default None)\n",
      "        Pass custom arguments to HDBSCAN.\n",
      "    \n",
      "    verbose: bool (Optional, default True)\n",
      "        Whether to print status data during training.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 15:07:10,359 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-07-04 15:08:57,842 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-07-04 15:09:54,017 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-07-04 15:11:01,804 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-07-04 15:11:08,477 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(documents=docs, \n",
    "                speed='deep-learn', ngram_vocab=(1,2),\n",
    "                workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17963"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[503 484 472 460 459 459 458 458 457 452 451 450 449 448 444 444 444 441\n",
      " 439 437 436 435 435 435 434 433 430 428 426 422 422 420 419 417 415 413\n",
      " 413 412 411 410 410 409 408 406 405 405 405 403 398 398 397 396 393 393\n",
      " 392 390 389 388 379]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic No. 0\n",
      "Words: ['jurassic park' 'head injury' 'street corner' 'packaging'\n",
      " 'saturday sunday' 'kopi lowak' 'cognitive function' 'mini riser' 'turner'\n",
      " 'outstanding' 'jet lag' 'appreciation' 'hunger' 'chooses' 'frenchman'\n",
      " 'river delta' 'different worlds' 'ship captain' 'sailing' 'hot coals'\n",
      " 'admitted' 'craft' 'golden' 'non identical' 'express feelings'\n",
      " 'having trouble' 'deserted' 'mothers' 'illegal immigrants' 'feeling sad'\n",
      " 'steven' 'co emissions' 'higher scores' 'diet products' 'lowest point'\n",
      " 'inches taller' 'ms' 'solving math' 'eggs' 'hopes' 'prize' 'centered'\n",
      " 'paired with' 'most powerful' 'your' 'electricity' 'boiling water'\n",
      " 'moods' 'restrictions' 'wild koalas']\n",
      "Topic No. 1\n",
      "Words: ['drawer' 'hat' 'red yellow' 'pointing out' 'stepped' 'taken aback'\n",
      " 'brown color' 'final analysis' 'uniform' 'tickets yuan' 'pill'\n",
      " 'drug addiction' 'sydney' 'autism' 'keep warm' 'very sad'\n",
      " 'asking permission' 'gaining' 'shut down' 'spoken english'\n",
      " 'waiting outside' 'motion picture' 'carnegie' 'concert'\n",
      " 'poisonous snakes' 'th grade' 'muddy water' 'colder' 'italy' 'states'\n",
      " 'wouldn let' 'exploration' 'toast' 'link' 'its creators' 'short' 'region'\n",
      " 'movie theatre' 'former' 'mr hunt' 'instant' 'crew' 'conflicts between'\n",
      " 'sighted' 'queen' 'grey squirrels' 'inability' 'mr burke'\n",
      " 'hunan satellite' 'anna jarvis']\n"
     ]
    }
   ],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "\n",
    "for words, scores, topic_num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(f\"Topic No. {topic_num}\")\n",
    "    print(f\"Words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Random Topics and Related Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic No.: 31\n",
      "Topic Keywords:\n",
      "['italians' 'tight' 'tv viewing' 'before bedtime' 'ride bicycle'\n",
      " 'scientific journal' 'relationship' 'whole family' 'mail addresses'\n",
      " 'allowed' 'doctors nurses' 'determine how' 'tropical rain' 'curled up'\n",
      " 'tv' 'american indians' 'experiencing' 'group members' 'most cases'\n",
      " 'binge drinkers' 'newspapers magazines' 'tom cruise' 'henderson island'\n",
      " 'properly understood' 'ms runkle' 'discipline' 'mauna loa' 'go shopping'\n",
      " 'ask questions' 'reading material' 'single sex' 'blows' 'firmly' 'saves'\n",
      " 'chinese netizens' 'dr smith' 'idiot savant' 'cartoon' 'main purpose'\n",
      " 'brain cancer' 'announced plans' 'weekend' 'best choice' 'au'\n",
      " 'specially designed' 'habitual loneliness' 'came back' 'should'\n",
      " 'presidents' 'magnifying glass']\n",
      "\n",
      "---------------\n",
      "Document: 3129, Score: 0.3379296660423279\n",
      "-----------\n",
      "Tokyo: The world's oldest man, retired Japanese silkworm breeder Yukichi Chuganji, died in his home at the age of 114, on Monday. Family members found him dead on his mattress. Born on March 23, 1889, Chuganji worked as a silkworm breeder and bank employee after leaving school. He also served as a community welfare  officer. He had been in god health, talking daily with his family members.\n",
      "Washington: Every American dislikes people who talk on cell phones while driving, even those who do that kind of act. In the State of New Jersey, 84 percent of 968 cell phone owners said in recent telephone survey that they would support a state ban  on the use of cell phones while driving. However, 42 percent of cell phone owners also said they used the devices \"very often\" or \"sometimes\" while driving. Although most agree that the banning is good, only 38 percent believed such a ban would be easy to enforce .\n",
      "New York: A woman in the US who was being attacked by a dog said she was saved from further harm when her 13-year-old daughter distracted the dog by screaming \"You want a piece of me?\" and kicked it repeatedly in the head. Jane Howell said she and her daughter, Elizabeth, were taking a walk around the neighborhood on Saturday evening when they came across he big dog, unchained.\n",
      "-----------\n",
      "\n",
      "Document: 10567, Score: 0.30980777740478516\n",
      "-----------\n",
      "White House Tours\n",
      "    Members of the public can visit the White House now. Requests   must be made through one's Member of Congress  . These self-guided tours are available from 7:30 am to 11:00 am Tuesday through Thursday, 7:30 am to 12:00 pm Fridays, and 7:30 am to 1:00 pm Saturdays (except for holidays). Requests can be made up to six months in advance and no less than 21 days in advance. All White House tours are free.\n",
      "    If you wish to visit the White House and are a citizen from a foreign country, please contact your embassy   for help in making a tour request.\n",
      "    All visitors 18 years of age or older will be required to present a valid   photo ID. All foreign visitors must present their passport. All other forms of foreign identification will not be accepted.\n",
      "    All visitors should call the 24-hour Visitors Office information line at 202-456-7041 to determine if any last minute changes have been made in the tour schedule  .\n",
      "    Forbidden Items:\n",
      "l    Cameras or video recorders\n",
      "l    Handbags, book bags, backpacks or purses\n",
      "l    Food, drinks, or cigarettes\n",
      "l    Any pointed objects\n",
      "l    Guns, or knives of any size\n",
      "    The U.S. Secret Service has the right to forbid any other personal items. Umbrellas, car keys, and cell phones (including those with cameras) are permitted. However, guests will not be allowed to use cell phones inside the White House.\n",
      "    Please note that there is no place to store forbidden items for visitors. Those who arrive with forbidden items will not be allowed to enter the White House.\n",
      "    Restrooms / Public telephones\n",
      "    The nearest restrooms and public telephones to the White House are in the Ellipse Visitor Pavilion (the park area south of the White House). Restrooms and public telephones are not available at the White House.\n",
      "    The Disabled\n",
      "    Please contact your Member of Congress if you cannot hear or see clearly and need help during your White House tour. Guide animals are permitted in the White House.\n",
      "-----------\n",
      "\n",
      "Document: 4743, Score: 0.2709919512271881\n",
      "-----------\n",
      "Rhinos  are big and can be dangerous. They are also shy and seldom seen. Once there were hundreds of rhino species, but today there are only five. One ancient rhino called Indricotherium was the largest land mammal that ever lived. It was 5m high at the shoulders, and 8.5m long-- twice the size of today's biggest elephant! The living rhinos are still big, averaging 2-3m long and weighing up to 3,600kg.\n",
      "To see all five species of living rhino you would need to do a bit of exploring. First you might visit Africa for a look at the white rhino and the black rhino. They live in Africa's dry woodlands and grasslands. Then you could head to India and Nepal for a look at the Indian rhino. It lives in the high grasslands near rivers, where you have to ride an elephant to find one. Finally you might travel to the rainforests on the islands of Indonesia to see the Sumatran and Javan rhinos. These are the smallest and rarest rhinos in the world and extremely difficult to find.\n",
      "Most rhinos are gentle and timid  .They have a bad reputation for being very\n",
      ", but that may be partly because they get frightened easily. Also, they have poor eyesight. They rely on their strong sense of smell to tell them a stranger is approaching, but if the wind is blowing the wrong way, they may not know someone is there until it's too late.\n",
      "Rhinos can be found only in their habitats, which is why habitat destruction has caused rhino populations to decline. As they disappear from certain places, their absence leads to many changes to the landscape. For example, black rhinos in Africa only eat shrubs   and small trees, pruning   the plants and limiting their growth. Many other species benefit from this, and as rhinos disappear, so do many other animals sharing their habitat. Bushes and trees take over the land and force species like the antelope to leave in search of food. In a short time the whole habitat has changed.\n",
      "-----------\n",
      "\n",
      "Document: 10391, Score: 0.26349154114723206\n",
      "-----------\n",
      "I was at a dance club when I was nineteen. One day my new friend walked with me to my car. It was a very cold night. As we were walking to my car, a man walked up to us. Behind him was a woman pushing  _ with a child inside who was about 2 years old. The child had only a jacket on and it wasn'tzipped . The man began to tell us he wanted to borrow some money for the night and that he had a job but no place to live and was waiting for his first paycheck. He confirmed he could get our mailing address and mail the money back to us.\n",
      "The guy I was with reached into his pocket to give this man a $20 bill. As the man was extending his hand out to take the money, I put my hand on my new friend's hand and said, \"Can I talk to you for a minute?\" I told him some people earned money by begging and that they always cheated those with soft hearts. And if they were truly worried about their child being out in the cold, they would have at least zipped his jacket.\n",
      "My friend looked at me with disapproval and said, \"Michael, I know there are some people out there that take advantage of others. I also know some people out there that are one paycheck away from being homeless. If I gave $20 to 10 people and only one of them really needed it and used it for the right thing, it was worth it.\"\n",
      "I am now thirty-seven years old and have never forgotten what he said to me. I don't even remember his name now. But I do remember that this experience changed my perspective .\n",
      "-----------\n",
      "\n",
      "Document: 17548, Score: 0.2591933608055115\n",
      "-----------\n",
      "Dear Carol,\n",
      "I wear braces   and I know I look just terrible in them. Is there anything I can do to improve my appearance until they come off?\n",
      "Connie N.\n",
      "Philadelphia, PA\n",
      "Dear Connie,\n",
      "Start by thinking about how good you are going to look when your braces come off. If you are negative about your appearance, people will pick that up right away about you. In the meantime there are several things that you can do to lift your spirits:\n",
      "Change your hairstyle. Be sure to get a good cut so that the shape of your hair will be flattering to your face and also good for your type of hair (fine, thick, straight, curly, etc.) Choose a hairstyle that draws attention away from your face. Hair pulled back into a pony-tail or into a ball on top or slightly off-center can be attractive and practical for this purpose.\n",
      "Keep skin clean. If you choose to wear makeup , it should look natural. Blusher  and lip gloss  in light tones of peach or pink are best.\n",
      "Be well-groomed . Neatness really _ . People see a total look about you before they ever become aware of your braces. And first impressions are lasting ones!\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "NUM_DOCS = 5 # No. of documents you wanna see for the Topic_Num\n",
    "TOPIC_NUM = random.randint(0, model.get_num_topics()-1)\n",
    "print(f\"Topic No.: {TOPIC_NUM}\")\n",
    "\n",
    "print(f\"Topic Keywords:\\n{model.get_topics(TOPIC_NUM+1)[0][-1]}\\n\")\n",
    "print(\"---------------\")\n",
    "\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=TOPIC_NUM, num_docs=NUM_DOCS)\n",
    "\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"../output/saved_models/top2vec_model_on_original_docs_{model.get_num_topics()}Topics\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top2Vec Model Using Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over the last 30 years, Bangkok, once a small fishing village, has transformed into a rich, concrete, high-rise city that it is today. The spreading metropolis and its population of 12 million now produces 35 per cent of Thailand's economic wealth.\n",
      "As a magnet for foreign companies, Bangkok attracts many overseas managers and business people from different fields, including tourism, automobiles and electronics. The city's population of foreigners is in the high hundreds of thousands, with tens of thousands of Japanese, Chinese and western employees working alongside hundreds of thousands of Burmese who mostly do unskilled jobs shunned by Thais.\n",
      "For those used to the good life, the variety and quality of the city's food is a key attraction, says one US manager, before listing many of his favourite Italian, Mexican and, of course, Thai restaurants. Most offer quality meals for less than the cost of a takeaway sandwich in London.\n",
      "Great choice and value can be found in Bangkok's other attractions, too. For overseas business people who enjoy shopping in luxury and air-conditioned comfort, the city has hundreds of modern shopping malls. Some foreigners, however, prefer the charms of Chatuchak Market, where anything can be bought at a good price by the skilled bargainer.\n",
      "When the time comes to talk business many overseas business people prefer to move out of the markets and onto the golf course. Thailand has thousands of courses, which can provide a welcome break from the busy and noisy city life. But most business people go to the golf course because it's the perfect place to discuss the next big deal.\n",
      "Because of the fast-paced life some foreign business people see Bangkok as a place to stay for the short term, rather than a lifetime. Australian computer software designer Sarah Huang is seven months pregnant but still working full-time in her Bangkok office. She says the city is \"definitely a place I want to stay for the next five, ten years\". Nannies and home help are affordable, but high fees for quality secondary education have convinced Ms. Huang to return to Australia when her child reaches high school age.\n"
     ]
    }
   ],
   "source": [
    "docs = df.document.tolist()\n",
    "print(docs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Top2Vec.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 15:20:05,573 - top2vec - INFO - Pre-processing documents for training\n",
      "c:\\Users\\Amit Vikram Raj\\#Self_Learning\\#ProjectPro\\Projects\\Topic-Modelling-Using-RACE-Dataset\\topic_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-07-04 15:20:20,230 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
      "2023-07-04 15:20:20,842 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-07-04 16:48:22,466 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-07-04 16:48:57,965 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-07-04 16:48:59,764 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(documents=docs, \n",
    "                speed='deep-learn',\n",
    "                embedding_model='all-MiniLM-L6-v2',\n",
    "                workers=-1) \n",
    "\n",
    "# embedding_model = 'all-MiniLM-L6-v2' or 'universal-sentence-encoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[601 554 418 349 347 322 321 302 296 267 265 257 256 250 247 240 237 236\n",
      " 235 233 229 228 223 222 219 217 208 205 201 199 199 198 197 196 192 191\n",
      " 189 189 185 183 177 176 167 167 160 157 156 156 155 148 146 145 145 144\n",
      " 142 137 133 131 128 128 127 127 127 127 127 126 126 125 124 124 123 121\n",
      " 121 121 121 117 116 115 113 112 111 111 110 110 109 109 108 107 107 106\n",
      " 105 105 104 103 102 102 102 100  99  97  97  93  92  91  89  89  89  89\n",
      "  88  88  87  86  85  85  85  84  81  81  81  80  79  79  79  79  79  79\n",
      "  78  77  77  76  76  76  76  75  74  74  74  73  72  71  71  71  71  69\n",
      "  68  68  68  67  67  66  66  62  62  62  61  61  60  60  59  59  59  58\n",
      "  57  57  57  57  56  56  56  56  55  55  54  52  52  52  52  51  51  51\n",
      "  49  49  48  47  47  46  46  46  46  46  45  44  44  43  42  42  42  41\n",
      "  41  41  39  39  39  37  37  36  36  36  36  35  34  34  33  33  31  29\n",
      "  27  26  26  26  26  24  22]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic No. 0\n",
      "Words: ['policeman' 'robber' 'incident' 'jokes' 'detective' 'sentences' 'puzzle'\n",
      " 'situation' 'funny' 'taxi' 'dickens' 'emergency' 'situations' 'robbed'\n",
      " 'accident' 'ambulance' 'policemen' 'troubles' 'passenger' 'neighbour'\n",
      " 'passengers' 'customer' 'police' 'mr' 'laughed' 'stole' 'patient'\n",
      " 'comedy' 'happened' 'sentence' 'story' 'acted' 'tense' 'suspect' 'ha'\n",
      " 'travelled' 'thief' 'laugh' 'clerk' 'car' 'circumstances' 'puzzles' 'mrs'\n",
      " 'joked' 'dialogue' 'behaviour' 'joke' 'waiter' 'beggar' 'stories']\n",
      "Topic No. 1\n",
      "Words: ['diet' 'meals' 'diets' 'eating' 'eat' 'nutrition' 'meal' 'obesity'\n",
      " 'foods' 'eats' 'food' 'nutritional' 'obese' 'lunches' 'healthy'\n",
      " 'healthier' 'consuming' 'consume' 'hunger' 'unhealthy' 'dinners' 'snack'\n",
      " 'lunch' 'consumption' 'dining' 'groceries' 'overweight' 'breakfast'\n",
      " 'feeding' 'fat' 'hungry' 'health' 'recipes' 'nutritious' 'snacks'\n",
      " 'appetite' 'nutrients' 'lunchtime' 'habits' 'lifestyles' 'fitness' 'fats'\n",
      " 'consumed' 'vegetables' 'cooking' 'fatty' 'dinner' 'diners' 'restaurants'\n",
      " 'calorie']\n"
     ]
    }
   ],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(2)\n",
    "\n",
    "for words, scores, topic_num in zip(topic_words, word_scores, topic_nums):\n",
    "    print(f\"Topic No. {topic_num}\")\n",
    "    print(f\"Words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic No.: 205\n",
      "Topic Keywords:\n",
      "['cloning' 'clone' 'reproduce' 'cloned' 'genetically' 'genetic' 'dna'\n",
      " 'biological' 'donor' 'adopt' 'adoption' 'copying' 'biology' 'transplant'\n",
      " 'breeding' 'creation' 'genes' 'copy' 'scientific' 'copies' 'darwin'\n",
      " 'biologist' 'recycling' 'copied' 'chimpanzees' 'gene' 'generation'\n",
      " 'evolution' 'scientist' 'donors' 'conservation' 'generations' 'debate'\n",
      " 'organisms' 'artificial' 'adopted' 'endangered' 'invention' 'donate'\n",
      " 'imitate' 'orphanage' 'animals' 'scientists' 'patent' 'extinction'\n",
      " 'flesh' 'mice' 'mammals' 'risks' 'innovation']\n",
      "\n",
      "---------------\n",
      "Document: 11756, Score: 0.8521753549575806\n",
      "-----------\n",
      "The scientific world continues to be amazed by the speed of the development of cloning. Some scientists now suggest that the cloning of humans could occur in the near future. Despite the benefits of cloning, however, certain ethical   questions concerning the possible abuse   of cloning have been raised. At the heart of these questions is the idea of humans influencing life in a way that could harm society, either morally or in a real physical sense. Some people object to cloning because it allows scientists to \"act like God\" in the handling of living organisms. \n",
      "   The cloning of Dolly raised the debate over this practice to a whole new level. It has become obvious that the technology for cloning Dolly could also be used to clone humans. A person could choose to make two or ten or a hundred copies of himself or herself by the same techniques used with Dolly. An active debate about the morality of cloning humans arises. Some people see benefits from the practice, such as providing a way for parents to produce a new child to replace one dying of a fatal disease. Other people worry about humans taking into their own hands the future of the human race. \n",
      "   At the beginning of the twenty-first century, many scientists say the controversy over the ethics of cloning humans is overstressed because of the unpredictability   of cloning in general. While scientists have cloned animals such as sheep, mice, cows, pigs, and goats, fewer than 3 percent of all those cloning efforts have succeeded. The animal clones that have been produced often have health problems. Scientists believe the rapid reprogramming in cloning can introduce random   errors into a clone's DNA. Those errors have altered individual genes in minor ways, and the genetic defects   have led to the development of major medical problems. Some scientists say this should make human cloning out of the question, but others argue that cloning humans may actually be easier and safer than cloning animals. Whatever, I agree that further research in the field of cloning is needed.\n",
      "-----------\n",
      "\n",
      "Document: 8780, Score: 0.8452054858207703\n",
      "-----------\n",
      "The researchers, led by Hwang Woo-suk, insist they cloned an Afghan hound, only to help investigate   human disease, including the possibility of cloning stem cells   for treatment purposes. \n",
      "But others immediately renewed calls for a global ban on human reproductive cloning before the technology moves any farther. \n",
      "\"Successful cloning of an increasing number of species confirms the general impression that it would be possible to clone any species of mammals, including humans,\" said Ian Wilmut, a reproductive biologist at the University of Edinburgh who produced the first cloned mammal, Dolly the sheep, from an adult cell nearly a decade ago. \n",
      "Researchers have since cloned cats, goats, cows, mice, pigs, rabbits, horses, deer, mules and gaur, a large wild ox of Southeast Asia. So far, efforts to clone a monkey or another primate with the same techniques have failed. \n",
      "Uncertainties about the health and life span   of cloned animals continue to exsist; Dolly died at a young age in 2003 after developing cancer and arthritis. \n",
      "Wilmut and others  _ Hwang's achievement, reported Wednesday in the journal Nature. But they said politicians and scientists must face the larger issue -- how to go on with the research without crossing the moral boundary of copying human life in the lab.\n",
      "\"The ability to use the technology is hopeful,\" said Robert Schenken, president of the American Society for Reproductive Medicine. \"However, the paper also points out that in dogs as in most species, cloning for reproductive purposes is unsafe.\" \n",
      "The cloned puppy was the lone success from more than 100 dogs implanted  with more than 1,000 cloned embryos. \n",
      "In a news conference in Seoul, the cloning team also condemned the reproductive cloning of humans as \"unsafe and inefficient.\" Human reproductive cloning already is banned in South Korea. Other nations, including the United States, are divided on whether to ban just human cloning or cloning of all kinds, including the production of stem cells.\n",
      "-----------\n",
      "\n",
      "Document: 11910, Score: 0.8309614658355713\n",
      "-----------\n",
      "Rome-Doctors and medical groups around the world last weekend reacted with strong opposition to the news that an Italian specialist is _ cloning the first human baby.\n",
      "    DL Severino Antinod,who is the head of a hospital in Rome,has been referred to in an Arab newspaper as claiming that one of his patients is eight weeks pregnant with a cloned baby.\n",
      "    Antinori refused to comment on the reports,but in March 2001 he said he hoped to produce a cloned embryo for implantation within two years.So far seven different kinds of mammals have already successfully cloned,including sheep,cats and most recently rabbits.\n",
      "Doctors showed their doubt and were strongly opposed although they admit that human cloning would finally come true unless there was a world wide ban on the practice.\n",
      "       Professor Rudolf Jaenisch of the Whitehead Institute for Biomedical Research at the Massachusetts Institute of Technology says,\"I find it astonishing that people do this where the result can be foretold that it will not be a normal baby.It is using humans as guinea pigs.It makes people feel sick.\"But Ronald Green,director of the Ethics institute at Darmouth College in the US,said it is unlikely that an eight-week-old pregnancy would lead to a birth.\n",
      "    So far all cloned animals have suffered from some different serious disorders,many of them dying soon after their births.\n",
      "    Doctors are opposed to human cloning because they are worried about the welfare of the cloned child if there is one.\n",
      "    \"There are no benefits of cloned human begins,just harm.\"said Dr.Michael Wilks of the UK.\n",
      "-----------\n",
      "\n",
      "Document: 11016, Score: 0.8308425545692444\n",
      "-----------\n",
      "Room doctors and medical groups around the world last weekend reacted with strong opposition to the news that an Italian specialist is  _ cloning the first human baby.\n",
      "         Dr. Severino Antinori, who is head of a hospital in Rome, has been referred to in an Arab newspaper claiming that one of his parents is eight weeks pregnant  with a cloned baby.\n",
      "         Antinori refused to comment on the reports, but in March 2001 he said he hoped to produce a cloned embryo  for implantation with two years. So far seven different kinds of animals have already successfully been cloned, including sheep, cats and most recently rabbits.\n",
      "         Doctors showed their doubt and were strongly opposed although they admitted that human cloning would finally come true unless there was a worldwide ban on the practice.\n",
      "         Professor Rudolf Jaenisch of the Whitehead Institute for Biomedical Research at the Massachusetts Institute of Technology said, \"I find it astonishing that people do this where the result can be foretold that it will not be a normal baby. It is using humans as guinea pigs . It makes people feel sick.\"\n",
      "         But Ronald Green, director of the Ethics Institute at Darmouth College in the USA, said it is unlikely that an eight-week-old pregnancy would lead to a birth.\n",
      "         So far all cloned animals have suffered from some different serious disorders, many of them dying soon after their birth.\n",
      "         Doctors are opposed to  human cloning because they are worried about the welfare of the cloned child if there is one. \"There are no benefits of cloned human beings, just harm,\" said Dr. Michael Wilks of the U.K.\n",
      "-----------\n",
      "\n",
      "Document: 4523, Score: 0.8297301530838013\n",
      "-----------\n",
      "Recently there was a major discovery in the scientific research--the mapping of all DNA in a human gene is complete. Couple of years ago, this seems an impossible task for scientist to accomplish. All this progress in science leads us to believe that the day, when the human being will be cloned, is not far away. Human cloning has always been a topic of argument,in terms of morality or religion.\n",
      "Taking a look at why cloning might be beneficial, among many cases, it is arguable that parents who are known to be at risk of passing a genetic limitation to a child could make use of cloning. If the clone was free of genetic limitations. Then the other clone would be as well. The latter could foe inserted in the woman and allowed to ripen to term. Moreover,cloning would enable women, who can't get pregnant, to have children of their own.\n",
      "Cloning humans would also mean that organs could be cloned, so it would be a source of perfect transfer organs. This, surely would be greatly beneficial to millions of unfortunate people around the world that are expected to lose their lives due to failure of single(or more) organ(s).It is also arguable that a ban on cloning may be unlawful and would rob people of the right to reproduce and limit the freedom of scientists.\n",
      "Arguments against cloning are also on a perfectly practical side. Primarily, I believe that cloning would step in the normal \"cycle\"of life. There would be a large number of same genes., which reduce the chances of improvement,and, in turn, development-the fundamental reason how living things naturally adapt to the ever-changing environment. Life processes failing to do so might result in untimely disappearance. Furthermore, cloning would make the uniqueness that each one of us possesses disappea. Thus, leading to creation of genetically engineered groups of people for specific purposes and, chances are, that those individuals would be regarded as \"objects\" rather than people in the society.\n",
      "Scientists haven't 100 percent. guaranteed that the first cloned will be normal. Thus this could result in introduction of additional limitations in the human \"gene-pool\".\n",
      "Regarding such arguable topics in \"black or white\" approach seems very innocent to me personally. We should rather try to look at all \"shades: of it. I believe that cloning is only legal if its purpose is for cloning organs, not humans. Then we could regard this as for \"saving life\" instead of \"creating life\". I believe cloning humans is morally and socially unacceptable.\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "NUM_DOCS = 5 # No. of documents you wanna see for the Topic_Num\n",
    "TOPIC_NUM = random.randint(0, model.get_num_topics()-1)\n",
    "print(f\"Topic No.: {TOPIC_NUM}\")\n",
    "\n",
    "print(f\"Topic Keywords:\\n{model.get_topics(TOPIC_NUM+1)[0][-1]}\\n\")\n",
    "print(\"---------------\")\n",
    "\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=TOPIC_NUM, num_docs=NUM_DOCS)\n",
    "\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"../output/saved_models/top2vec_model_on_original_docs_all-MiniLM-L6-v2_{model.get_num_topics()}Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
