{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b594282-b74f-45c6-a869-ce5d88955788",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766ca46-c42d-404c-8365-1e2f7c0ccee2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. What is LDA?  [Source: Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Machine_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc0836-7c4b-4083-9998-89dd53d930ea",
   "metadata": {},
   "source": [
    "- In NLP, LDA is a (bayesian)statistical method to discover topics in a collection of documents, and then automatically classify any individual document within the collection in terms of how \"relevent\" it is to each of the discovered topics.\n",
    "\n",
    "- A **topic** ***is considered to be a set of terms (i.e., individual words or phrases)that, taken together, suggest a shared theme.***\n",
    "\n",
    "    - For example, in a document collection related to pet animals, the terms dog, spaniel, beagle, golden retriever, puppy, bark, and woof would suggest a **DOG_related** theme, while the terms cat, siamese, Maine coon, tabby, manx, meow, purr, and kitten would suggest a **CAT_related** theme. There may be many more topics in the collection - e.g., related to diet, grooming, healthcare, behavior, etc. that we do not discuss for simplicity's sake.<br></br>\n",
    "\n",
    "- If the document collection is sufficiently large, LDA will discover such sets of terms (i.e., topics) based upon the co-occurrence of individual terms, **though the task of assigning a meaningful label to an individual topic (i.e., that all the terms are DOG_related) is up to the user.**\n",
    "\n",
    "- Also LDA is a generative model â€” it tries to determine the underlying mechanism that generates the articles and the topics.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c0901-bbb5-4263-89e9-a7122599d19d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. The LDA approach assumes that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d28f-d762-4041-ab15-7a93bff26107",
   "metadata": {},
   "source": [
    "1. [The distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics): **Words that appear together frequently are likely to be close in meaning**\n",
    "\n",
    "2. **Each topic is a mixture of different words** <br></br> In below fig., notice that the topic \"Health & Medicine\" has various words associated with it to varying degrees (\"cancer\" is more strongly associated than \"vascular\" or \"exercise\"). Note that different words can be associated with different topics, as with the word \"cardio\".\n",
    "\n",
    "    <div align=\"center\">\n",
    "        <img src=\"images/lda_1.png\" width='500' title=\"Topic as as mixture of words\" />\n",
    "    </div>\n",
    "\n",
    "3. **Each document is a mixture of different topics** <br></br> In below fig., we can see that a single document can pertain to multiple topics (as colour-coded on the left). Words like \"injury\" and \"recovery\" might also belong to multiple topics.\n",
    "\n",
    "    <div align=\"center\">\n",
    "        <img src=\"images/lda_2.png\" width='500' title=\"Document as a mixture of topics\" />\n",
    "    </div>\n",
    "    \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333ed37-dbb7-48a9-b0ad-c97716217033",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. How does LDA Works? (Intutive Explanation!)\n",
    "\n",
    "- Watch these videos especially part-1 to understand the explanation\n",
    "\n",
    "    - [Latent Dirichlet Allocation (Part 1 of 2) - Serrano.Academy](https://youtu.be/T05t-SqKArY)\n",
    "\n",
    "    - [Training Latent Dirichlet Allocation: Gibbs Sampling (Part 2 of 2) - Serrano.Academy](https://youtu.be/BaM1uiCpj_E)\n",
    "\n",
    "\n",
    "- Very nice Medium article\n",
    "\n",
    "    - [Latent Dirichlet Allocation - Ioana](https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cf25d-467f-42be-b77b-ea9b285154a8",
   "metadata": {},
   "source": [
    "- **The Problem:** The problem is we have collection of documents and we want to sort them into topics. Also we don't know what those topics are.\n",
    "\n",
    "    - For Simplicity: let's say our collection of docs has only 4 docs, and each doc. only has 5 words and our language has only 4 words - \"ball\", \"galaxy\", \"planet\", \"referendum\". ANd we have 3 possibilities for our topics namely, \"Science\", \"Sports\", \"Politics\"\n",
    "    \n",
    "    <div align=\"center\">\n",
    "    <img src=\"images/lda_docs.png\" width=\"500\"/>\n",
    "    </div>\n",
    "    \n",
    "This is where LDA comes into play, if you want to classify these docs. into 3 broad topics, LDA takes a geometric approach and build a triangle with each topic at it's corner and then it puts all the documents inside that triangle in a way such that documents are close to the topic(s) they closely resemble to.\n",
    "\n",
    "For e.g., let's say we have 3 topics namely, \"Science\", \"Sports\", \"Politics\" for a collection of docs(or news-articles):\n",
    "<div>\n",
    "    <img src=\"images/lda_3.png\" width=\"500\"/>\n",
    "    <img src=\"images/lda_4.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "As we can see that, docs related to only single topic are at the corners while the documents relating to more than one topic are somewhat far from corners or in the middle or on the edge of the triangle.\n",
    "\n",
    "Now, the question is How do we put the articles inside the triangle in a reasonable way?\n",
    "\n",
    "Think of LDA as a machine that generates documents. This machine has some settings(that we can play with), gears and a button. When we press that button then these starts turning and build a document. Now the most likely the doc that comes out is gibberish, just a bunch of words put together, this is what we call **topic** as defined above. From this collection of words, a user will assign a meaningful label to the individual topic produced by the machine. \n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_5.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "In LDA we take bunch of these machines with diff. settings and selects the one which generates the most meaningful document(collection of words) to get our topics.\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_6.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089738d4-22ad-4138-a0ba-7738c87f621a",
   "metadata": {},
   "source": [
    "**Now in order to understand how the machine works, let's look at it's blue-print**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_7.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Below formula represents the probability of a document that the machine spits out:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_8.png\" width=\"500\"/>\n",
    "</div>\n",
    "The first two terms can be understood as the settings of the machine and last two as the gears. The 1st and 3rd term will help us to find the topics and 2nd and 4th will help us find words in the article.\n",
    "\n",
    "\n",
    "Now, let's first understand what is [**Dirichlet Distributions**](https://en.wikipedia.org/wiki/Dirichlet_distribution)?\n",
    "\n",
    "Dirichlet distributions can be imagined as a triangle with bunch of points in it. It has parameter $\\alpha$ which controls the distribution of points in the triangle as shown below:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_9.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b3179-8de2-4141-a873-fa3a680b7f0a",
   "metadata": {},
   "source": [
    "Now imagine this triangle w.r.to our articles as points in the triangle and topics on the corners of the triangle. It is evident that the middle one$(\\alpha < 1)$ will represent the dirichlet distribution for articles w.r.to topics, and we'll get a probability for each doc w.r.to all 3 topics:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_10.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Imagine Dirichlet distributions as a distribution of distributions, as every point inside the triangle gives us some combination of red, green and blue, later we'll link this to multinomial distribution:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_11.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "- Now, an obvious question arises, **What if we have more than 3 topics?**\n",
    "    - For 2-points/topics we can represent them with a line\n",
    "    - For 3, a triangle\n",
    "    - For 4 topics, we might think to use a square but in a square the diagonal is longer than the sides and we want points to be equi-distant from each other, so we use **tetrahedron**, this tetrahedron has equilateral triangle as it's faces.\n",
    "    - For n-topics we'll use something called [**n-dimensional simplex.**](https://en.wikipedia.org/wiki/Simplex)\n",
    "    \n",
    "    - For n=4, these dirichlet distributions will exists in 3-dimensional space.\n",
    "    <div align=\"center\">\n",
    "    <img src=\"images/lda_12.png\" width=\"600\"/>\n",
    "    <img src=\"images/lda_13.png\" width=\"400\"/>\n",
    "    </div>\n",
    "    \n",
    "\n",
    "Now, let's imagine another dirichlet distribution, slightly diff. than above(topic one), where the corners represents the words in our vocabulary, and the points inside it represents the topics. Now in this case for each topic/point inside the tetrahedron we'll have probability distribution w.r.to all the words as shown below:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_14.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218d21d-8ae8-4955-bbf0-c258351f9159",
   "metadata": {},
   "source": [
    "So, to summarize now we have two dirichlet distributions, \n",
    "- the left one associates documents with their corresponding topics and,\n",
    "- the one on the right maps topics with their corresponding words\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_15.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "* **\n",
    "\n",
    "**Now, let's put all of this together:**\n",
    "\n",
    "1. Remember, that LDA is a machine that produces documents and this machine has some settings that we adjust, these are the two dirichlet distributions above and the way we adjust the settings is by moving the points inside the dist., then we press the button and activate the gears producing a document.\n",
    "\n",
    "2. Now we'll randomly select a document from our document-topic triangle and obtain the probabilities of that document belonging to the one of the three topics, as shown in the 1st figure.\n",
    "\n",
    "3. From those probabilities we'll create a multinomial distribution, think of it as a box filled with 3 diff. colored balls with the respective probability of obtaining them as show below in the 3rd figure. Now we'll randomly pick topics from this box one by one.\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_16.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "4. With all of these random topics, we'll pick words from the 2nd topic-word dirichlet distribution as shown below. And for all 3-topics we obtain a prob. distribution of words from the topic-word dirichlet distribution and with these we'll create a multinomial distribution of words as shown below.\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_17.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "5. Now from each randomly selected topic in (3.) we'll randomly pick respective words from the obtained multinomal dist. above.\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_18.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Finally, we put together all the obtained words, we get our \"topic\" for the document passed in (1.). Remember above we defined a \"topic\" as a collection of words that represent a shared theme. So, we go back to our original document and match if the words outputed by the machine matches the theme of the document and, if yes, then a user assigns a meaningful label to the outputed topic.\n",
    "\n",
    "Finally, we do the same for all the documents and see if the topics closely resemble the theme of the documents.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_19.png\" width=\"500\"/>\n",
    "    <img src=\"images/lda_20.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "<!-- $$\\large{\\prod_{j=1}^M P(\\theta_{j}; \\alpha)}$$\n",
    "\n",
    "$$\\large{\\prod_{j=1}^M P(\\varphi_{j}; \\beta)}$$\n",
    "\n",
    "$$\\large{\\prod_{j=1}^M P(Z_{j,t} | \\theta_j)}$$\n",
    "\n",
    "$$\\large{\\prod_{j=1}^M P(W_{j,t} | \\varphi_{Z_{j,t}})}$$ -->\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b91f74-5aff-4a23-8d9f-2a764cef4723",
   "metadata": {},
   "source": [
    "In context of our machine analogy,\n",
    "- The two dirichlet distributions represents the settings of the machine with the arrangement of points inside them.\n",
    "\n",
    "- The two obtained multinomial distributions can be thought of as the gears of the machine that come out of the dirichlet distributions when we press the start button.\n",
    "\n",
    "- We also talked about taking multiple of these machines and selecting the best one, which is basically all the possible arrangement of points inside the dirichlet distributions.\n",
    "\n",
    "\n",
    "Also, let's relate everything with the blue-print of LDA:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/lda_21.png\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030652c-73a0-4a30-8dab-9afe8e4ab8f7",
   "metadata": {},
   "source": [
    "## LDA - Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6f9c2-976d-4b98-b4f6-62d3c23408be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89303b3a-f5cc-4964-a687-f9994724c584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
