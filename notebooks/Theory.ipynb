{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5eecd8-ae9a-470c-96af-a04b9ca564e6",
   "metadata": {},
   "source": [
    "Random Stuff:\n",
    "\n",
    "### Why PoS(Parts of Speech) tagging in needed in text preprocessing?\n",
    "\n",
    "- *Apple* is a great company? $\\rightarrow$ Here \"Apple\" is a **Proper Noun**\n",
    "- Did you eat that *Apple*? $\\rightarrow$ Here \"Apple\" is a **Common Noun**\n",
    "\n",
    "So, here \"Apple\" word has different context in both cases. And that is why PoS tagging is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1eed99-fd84-49cc-aefe-e014a86e4556",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latent Semantic Analysis(LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f3322-1ef4-4bc1-8196-3f9f3e4fc34f",
   "metadata": {},
   "source": [
    "### **Intutuion behind LSA:**\n",
    "\n",
    "When we write anything like text, the words are not chosen randomly from a vocabulary.\n",
    "\n",
    "Rather, we think about a theme (or topic) and then chose words such that we can express our thoughts to others in a more meaningful way. This theme or topic is usually considered as a latent dimension.\n",
    " \n",
    "It is latent because we can’t see the dimension explicitly. Rather, we understand it only after going through the text. This means that most of the words are **semantically linked** to other words to express a theme. So, if words are occurring in a collection of documents with varying frequencies, it should indicate how different people try to express themselves using different words and different topics or themes.\n",
    "\n",
    "In other words, word frequencies in different documents play a key role in extracting the latent topics. \n",
    "\n",
    "- *One Line Definition of What LSA is:*\n",
    "    - ***LSA tries to extract the underlying theme/context/topics present in the text documents using Singular Value Decomposition(SVD).***\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c264e73-ebcd-4fad-88a8-60b1f8f6da36",
   "metadata": {},
   "source": [
    "### **LSA - Wikipedia**\n",
    "\n",
    "- Latent semantic analysis (LSA) is a technique in NLP, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts/topics related to the documents and terms. \n",
    "\n",
    "- LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). \n",
    "\n",
    "    - The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings.\n",
    "    \n",
    "    - The underlying idea that \"a word is characterized by the company it keeps\" was popularized by Firth in the 1950s<br></br>\n",
    "\n",
    "- A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Documents are then compared by cosine similarity between any two columns. Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.\n",
    "\n",
    "\n",
    "* **\n",
    "\n",
    "- LSA uses a **document-term matrix** which describes the occurrences of terms in documents. It is a sparse matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents and whose columns correspond to terms. It is also common to encounter the transpose, or **term-document matrix** where documents are the columns and terms are the rows. \n",
    "\n",
    "\n",
    "- In practice, however, raw counts do not work particularly well because they do not account for the significance of each word in the document.\n",
    "\n",
    "- Instead of simply using frequency of terms in the matrix, we can weight the raw counts using **tf-idf**(term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34cad9f-94d9-41a8-9d62-626d7fde1926",
   "metadata": {},
   "source": [
    "### **How does LSA works?**\n",
    "\n",
    "1. LSA creates a document-term matrix or a term-document matrix. $$\\large{X_{m \\times n}} \\rightarrow \\normalsize{\\text{term-document matrix}}$$ where $n=\\text{# of documents; } m = \\text{# of unique terms}$, vice-versa in case of document-term matrix.\n",
    "\n",
    "<!-- ![Latent-Semantic-Analysis](images/lsa_1.png) -->\n",
    "<div align='center'>\n",
    "    <img src=\"images/lsa_1.png\" width='1000'>\n",
    "</div>\n",
    "\n",
    "\n",
    "> The dot product $\\textbf{t}_i^T\\textbf{t}_p$ b/w the two terms vectors **gives the correlation b/w the terms over the set of documents**. The matrix product $\\large{XX^T}$ contains all these dot products, and matrix $\\large{XX^T}$ is a symmectric i.e. $\\textbf{t}_i^T\\textbf{t}_p = \\textbf{t}_p^T\\textbf{t}_i$.\n",
    "\n",
    "> Similarly the matrix $\\large{X^TX}$ contains dot products b/w all the document vectors, **giving their correlation over the terms**. The matrix $\\large{X^TX}$ is also symmetric i.e $\\textbf{d}_j^T\\textbf{d}_q = \\textbf{d}_q^T\\textbf{d}_j$.\n",
    "\n",
    "\n",
    "\n",
    "2. By **Singular Value Decomposition(SVD)**, any $m \\times n$ matrix can be decomposed into three matrices as: $$\\large{X = U \\Sigma V^{T}}$$ where $\\large{U_{m \\times m}}$ and $\\large{V_{n \\times n}}$ are orthogonal matrices and $\\large{\\Sigma_{m \\times n}}$ diagonal matrix, not necessarily square.\n",
    "\n",
    "    - The elements along the diagonal of $\\large{\\Sigma_{m \\times n}}$ are known as the **singular values of the matrix $\\large{X}$**, which are square-root of eigen values of matrix $\\large{XX^T}$.\n",
    "\n",
    "    - The columns of $\\large{U_{m \\times m}}$ are known as the **left-singular vectors**, which are eigen-vectors of $\\large{XX^T}$.\n",
    "\n",
    "    - The columns of $\\large{V_{n \\times n}}$ are known as as the **right-singular vectors**, which are eigen-vectors of $\\large{X^TX}$.\n",
    "    \n",
    "    - The sigular values in diagonal matrix and singular vectors in U & V are arranged in the ascending order of sigular values.<br></br>\n",
    "    \n",
    "\n",
    "3. It turns out that when you select the $\\large{k}$ $(k << m)$ largest singular values, and their corresponding singular vectors from $\\large{U}$ and $\\large{V}$, you get the rank $\\large{k}$ approximation to $\\large{X}$ with the smallest error. This approximation has a minimal error. Also we can now treat the term and document vectors as a [\"semantic space\"](https://en.wikipedia.org/wiki/Semantic_space).  $$\\large{X_{k} = U_{k} \\Sigma_{k} V_{k}^{T}}$$\n",
    "\n",
    "$$\\large{X_{k} = U_{m \\times k} \\Sigma_{k \\times k} V_{k \\times n}^{T}}$$\n",
    "\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b7e4c-3ee5-428e-9119-29758806ba9b",
   "metadata": {},
   "source": [
    "### Interpretation of Mathematical Symbols\n",
    "\n",
    "4. **This is what LSA does:** Essentially we have reduced the dimenions of term and document vectors. \n",
    "\n",
    "    - Each row in the matrix $\\large{U_{k}}$, which is a $m \\times k$ matrix, **represents term vector** reduced to $k$ dimensions.\n",
    "    \n",
    "        - **The Matix $\\large{U}$ is called term-topic matrix**, whose each value represents relation b/w each term and (latent)topic within the document i.e. how closely that particular term is related to the topic. Values close to 1 represent high correlation b/w them.<br></br>\n",
    "    \n",
    "    - Each column in the $\\large{V_{k}^{T}}$, which is a ${k \\times n}$ matrix, **represents document vector** reduced to $k$ dimensions.\n",
    "    \n",
    "        - **The Matix $\\large{V}$ is called document-topic matrix**, whose each value represents relation b/w each document and (latent)topic within the document i.e. how closely that particular text/doc. is related to the topic. Values close to 1 represent high correlation b/w them.<br></br>\n",
    "        \n",
    "        \n",
    "    - The Matrix $\\large{\\Sigma}$ represents the correlation b/w each identified (latent)topics.\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"images/lsa_2.png\" width='1000'>\n",
    "</div>\n",
    "\n",
    "\n",
    "- For visual understanding [Read this article](https://www.geeksforgeeks.org/latent-semantic-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353d0fb-490a-44d3-a749-26f4fa372e2e",
   "metadata": {},
   "source": [
    "### Application of Document and Term Vectors\n",
    "\n",
    "Now with the help of these document vectors and term vectors, we can easily calculate some measures such as cosine similarity to evaluate:\n",
    "\n",
    "1. The similarity of different documents.\n",
    "2. The similarity of different words.\n",
    "3. The similarity of terms or queries and documents which will become useful in information retrieval, when we want to retrieve passages most relevant to our search query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa09c06-634c-4bae-bfbd-6bb710087d05",
   "metadata": {},
   "source": [
    "### Code-Implementation of LSA\n",
    "\n",
    "[Databricks Academy - LSA](https://youtube.com/playlist?list=PLroeQp1c-t3qwyrsq66tBxfR6iX6kSslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62db77-789a-4a07-be0c-732078900ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18147ce9-840f-447d-8310-594999e92760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7051c5-8c11-4d16-ad67-354e755356fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631faec-112f-4255-bd4d-ff46c398caae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5a997-b068-4585-a761-4a4eca333ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15d996d9-fc7c-4bee-84da-28342f1c0f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-12T07:41:44.863580Z",
     "iopub.status.busy": "2023-06-12T07:41:44.863580Z",
     "iopub.status.idle": "2023-06-12T07:41:44.886475Z",
     "shell.execute_reply": "2023-06-12T07:41:44.885460Z",
     "shell.execute_reply.started": "2023-06-12T07:41:44.863580Z"
    },
    "tags": []
   },
   "source": [
    "- References:\n",
    "    1. [Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis)\n",
    "    2. [TDS](https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8)\n",
    "    3. [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/09/latent-semantic-analysis-and-its-uses-in-natural-language-processing/)\n",
    "    4. [GFG](https://www.geeksforgeeks.org/latent-semantic-analysis/)\n",
    "    5. [Databricks Academy - LSA](https://youtube.com/playlist?list=PLroeQp1c-t3qwyrsq66tBxfR6iX6kSslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13b7bf-3b56-40d0-b254-b2d9a766fae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
